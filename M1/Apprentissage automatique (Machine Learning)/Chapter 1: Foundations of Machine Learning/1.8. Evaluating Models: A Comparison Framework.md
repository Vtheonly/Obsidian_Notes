  

# 1.8 Evaluating Models: A Comparison Framework

## Context

Once a model is trained, we need a reliable way to measure its performance and compare it to other models. Simply looking at the error on a test set can be misleading due to statistical randomness. A more robust framework involves using statistical tools like confidence intervals.

---

## The Role of the Test Set

The primary goal of evaluation is to estimate a model's **generalization error**—its expected error on new, unseen data drawn from the true data distribution. The error calculated on a finite **test set** is an *estimate* of this true error.

Because the test set is a random sample, this estimate has inherent uncertainty. A different test set would likely produce a slightly different error score.

> [!Warning] A Single Number is Not Enough
> Reporting a single accuracy score (e.g., "92.5% accuracy") can be deceptive. Due to random chance in the test set, this number could be higher or lower than the model's true average performance.

---

## Confidence Intervals for Robust Comparison

A **confidence interval** provides a range of values that is likely to contain the true performance metric (e.g., true error rate) of a model with a certain level of confidence (typically 95%).

It is calculated using the mean error observed on the test set and the **standard error** of that mean.

`Confidence Interval = Mean Error ± (Critical Value * Standard Error)`

For a 95% confidence level, the critical value is approximately 1.96. The standard error decreases as the size of the test set (`N`) increases, meaning larger test sets lead to tighter, more precise confidence intervals.

---

## Comparing Two Algorithms

Confidence intervals are invaluable for comparing two models, A and B.

* **No Overlap**: If the 95% confidence interval for model A's error is `[0.10, 0.12]` and for model B's is `[0.14, 0.16]`, their intervals do not overlap. We can be reasonably confident that model A is statistically superior to model B.

* **Overlap**: If the intervals for A and B overlap (e.g., `[0.10, 0.14]` and `[0.12, 0.16]`), the observed difference in their mean performance might be due to random chance. We cannot confidently conclude that one is better than the other.

| Key Metric | Description |

| ------------------- | ------------------------------------------------------------------------------------------------------------- |

| **Mean Error** | The average error (e.g., classification error) calculated on the test set. |

| **Standard Error** | An estimate of the standard deviation of the mean error. It quantifies the uncertainty of the mean estimate. |

| **Confidence Interval** | A range that captures the true error with a specified probability (e.g., 95%). |

> [!Note] Best Practice

> When comparing models, always report confidence intervals alongside point estimates of performance. This provides a much clearer and more honest picture of the relative strengths of the models.