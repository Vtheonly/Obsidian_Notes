  

# 1.5 Controlling Capacity with Regularization

## Context

While adjusting a model's structural complexity (like the degree `M` of a polynomial) is one way to combat overfitting, it is often impractical. **Regularization** provides a more flexible method to control a model's effective capacity without changing its underlying architecture.

---

## The Principle of Regularization

Regularization works by adding a **penalty term** to the error function that the model is trying to minimize. This penalty discourages the model from becoming too complex by constraining the magnitude of its parameters (`w`).

The modified error function takes the form:

`Total Error = Data Error + λ * Complexity Penalty`

Here, `λ` (lambda) is a hyperparameter that controls the strength of the regularization.

---

## L2 Regularization (Weight Decay)

A very common form of regularization is **L2 regularization**, also known as *weight decay* or *Ridge regression*. It adds a penalty proportional to the sum of the squared values of the model's weights.

The penalty term is: `Ω(w) = 1/2 * ||w||² = 1/2 * Σ(w_j²)`

> **Intuition**: In the polynomial regression example, overfitting (e.g., with M=9) was characterized by extremely large positive and negative weight values, causing wild oscillations. By penalizing large weights, L2 regularization forces the model to find a "simpler" solution where the curve is smoother, thus improving generalization.

---

## The Role of the Hyperparameter λ

The regularization coefficient `λ` controls the trade-off between fitting the training data and keeping the model simple.

* **If `λ` is very large**: The penalty for large weights dominates. The model is forced to have very small weights, effectively reducing its capacity. This can lead to **underfitting**.

* **If `λ` is very small (or zero)**: The penalty is negligible. The model is free to fit the training data as closely as possible, which can lead to **overfitting**.

Finding the optimal value for `λ` is a crucial part of the model selection process, often done using a validation set.

| Lambda (`λ`) | Effect | Risk |

| ------------ | -------------------------------------- | ------------------- |

| **Small** | Low penalty, high model complexity | Overfitting |

| **Large** | High penalty, low model complexity | Underfitting |

> [!Note] Good Practice

> Regularization is a powerful technique for preventing overfitting and is a standard component in the training of many modern machine learning models. It helps create models that are more robust and generalize better to new data.

***